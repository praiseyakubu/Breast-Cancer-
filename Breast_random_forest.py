# -*- coding: utf-8 -*-
"""Assignment random forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X7WmN732KMEn3jThiNlqkZYOFFp4WQ_D
"""

# Data Preparation
from pandas import read_csv, get_dummies,Series,DataFrame
#import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
data=read_csv('/content/Breast_Cancer (1).csv')# reading
data['Status'] = data['Status'].map({'Alive':1, 'Dead':0})
data['Progesterone Status'] = data['Progesterone Status'].map({'Positive':1, 'Negative':0})
data['A Stage'] = data['A Stage'].map({'Regional':1, 'Distant':0})
data['Estrogen Status'] = data['Estrogen Status'].map({'Positive':1, 'Negative':0})
categorical_features = ['Race', 'Marital Status', 'T Stage ', 'N Stage', '6th Stage', 'differentiate', 'Grade']
data2 = get_dummies(data, columns = categorical_features, drop_first=True)
X = data2.drop('Status', axis = 1) # Features
Y = data2['Status'] # Labels
X_scaled = StandardScaler().fit_transform(X) # scaling

X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.2, random_state = 100)# splitting
X_train,Y_train =SMOTE (random_state = 100).fit_resample(X_train,Y_train)# balancing

# Random Forest Classifier (method 1)
#from sklearn.ensemble import RandomForestClassifier
from sklearn import ensemble
#RF_classifier1 =ensemble.RandomForestClassifier()
RF_classifier1 =ensemble.RandomForestClassifier(n_estimators=50, criterion='entropy', max_features='auto', random_state=1)  # building model
RF_classifier1.fit(X_train,Y_train)#training
Y_pred1=RF_classifier1.predict(X_test)# testing
# imp_features = Series(RF_classifier1.feature_importances_, index=list(X)).sort_values(ascending=False)
# print(imp_features)

# Evaluation
# Acuracy and confusion matrix
from sklearn import metrics
Accuracy=metrics.accuracy_score(Y_test, Y_pred1) # calculating accuaracy
print("Accuracy: ", Accuracy) # Is this a good metric??
con_matrix = metrics.confusion_matrix(Y_test, Y_pred1)
print (con_matrix)
recall = metrics.recall_score(Y_test, Y_pred1)
print (recall)
percision=metrics.precision_score(Y_test, Y_pred1)
print(percision)

# Random Forest Classifier (method 2)
from sklearn.model_selection import GridSearchCV
RF_classifier2 = ensemble.RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1) # building model
no_trees = {'n_estimators': [150,50, 500, 250, 400, 450]}
grid_search1 = GridSearchCV(estimator=RF_classifier2, param_grid=no_trees, scoring='recall', cv=5)
grid_search1.fit(X_scaled, Y)## training, testing , evaluation, ranking.
best_parameters = grid_search1.best_params_
print(best_parameters)
best_result = grid_search1.best_score_
print(best_result)

# Random Forest Classifier with best number of tree (method 1)

RF_classifier3 = ensemble.RandomForestClassifier(n_estimators=400, criterion='entropy', max_features='auto', random_state=1)# building model
RF_classifier3.fit(X_train,Y_train) #training
Y_pred3=RF_classifier3.predict(X_test)# testing
imp_features = Series(RF_classifier3.feature_importances_, index=list(X)).sort_values(ascending=False)
print(imp_features)

# Using important features only (method #2)
categorical_features = ['Race', 'Marital Status', 'T Stage ', 'N Stage', '6th Stage', 'differentiate', 'Grade']
data2 = get_dummies(data, columns = categorical_features, drop_first=True)
# X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.3, random_state = 100)# splitting
# X_train,Y_train =SMOTE (random_state = 100).fit_resample(X_train,Y_train)# balancing

RF_classifier4 = ensemble.RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1) # building classifier
no_trees = {'n_estimators': [200, 250, 300, 350, 400, 450]}
grid_search2 = GridSearchCV(estimator=RF_classifier4, param_grid=no_trees, scoring='recall', cv=5)
grid_search2.fit(X_scaled, Y) # training, testing , evaluation, ranking.

best_parameters = grid_search2.best_params_
print(best_parameters)
best_result = grid_search2.best_score_
print(best_result)

# Using pipeline (method #3)
from imblearn.pipeline import Pipeline
model0 = Pipeline([('balancing', SMOTE(random_state = 101)),
        ('classification', ensemble.RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1) )# building classifier
    ]) # building classifier
no_trees = {'classification__n_estimators': [10,20,30,40,50,100]}
grid_search3 = GridSearchCV(estimator=model0, param_grid=no_trees, scoring='precision', cv=5)
grid_search3.fit(X_scaled, Y)

best_parameters = grid_search3.best_params_
print(best_parameters)
best_result = grid_search3.best_score_
print(best_result)
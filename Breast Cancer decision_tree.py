# -*- coding: utf-8 -*-
"""Assignment decision Tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vN-ria8Kh4g8vIjZ_fARbMaxkB_GmYfw
"""

# Data Preparation
from pandas import read_csv, get_dummies,Series,DataFrame
#import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
data=read_csv('/content/Breast_Cancer (1).csv')# reading
data['Status'] = data['Status'].map({'Alive':1, 'Dead':0})
data['Progesterone Status'] = data['Progesterone Status'].map({'Positive':1, 'Negative':0})
data['A Stage'] = data['A Stage'].map({'Regional':1, 'Distant':0})
data['Estrogen Status'] = data['Estrogen Status'].map({'Positive':1, 'Negative':0})
categorical_features = ['Race', 'Marital Status', 'T Stage ', 'N Stage', '6th Stage', 'differentiate', 'Grade']
data2 = get_dummies(data, columns = categorical_features, drop_first=True)
X = data2.drop('Status', axis = 1) # Features
Y = data2['Status'] # Labels
X_scaled = StandardScaler().fit_transform(X) # scaling

X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.2, random_state = 100)# splitting
X_train,Y_train =SMOTE (random_state = 100).fit_resample(X_train,Y_train)# balancing

# Decision Tree Classifier (method#1)
from sklearn import tree
#from sklearn.tree import DecisionTreeClassifier
#import sklearn as  sk
DT_classifier = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 5)# classifier building
DT_classifier.fit(X_train, Y_train) # training
Y_pred = DT_classifier.predict(X_test) # testing
# imp_features =Series(DT_classifier.feature_importances_, index=list(X)).sort_values(ascending=False) # what are the important features?
# print(imp_features)

# Evaluation
# Acuracy and confusion matrix
from sklearn import metrics
Accuracy=metrics.accuracy_score(Y_test, Y_pred) # calculating accuaracy
print("Accuracy: ", Accuracy) # Is this a good metric??
con_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print (con_matrix)
recall = metrics.recall_score(Y_test, Y_pred)
print (recall)
percision=metrics.precision_score(Y_test, Y_pred)
print(percision)

#using GridSearch (method#2)
from sklearn.model_selection import GridSearchCV
DT_classifier2 = tree.DecisionTreeClassifier(criterion = 'entropy') # building classfier
depth = {'max_depth': [2,3,4,5,10,15,20,25,30,35]}
grid_search1 = GridSearchCV(estimator=DT_classifier2, param_grid=depth, scoring='precision', cv=5)# building
grid_search1.fit(X_scaled, Y )# training, testing , evaluation, ranking.
best_depth = grid_search1.best_params_
print(best_depth)
best_result = grid_search1.best_score_
print(best_result)

# Decision Tree Classifier (method#1) with the best depth
from sklearn import tree
DT_classifier3 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 25)# classifier building
DT_classifier3.fit(X_train, Y_train) # training
Y_pred2 = DT_classifier3.predict(X_test) # testing
imp_features = Series(DT_classifier3.feature_importances_, index=list(X)).sort_values(ascending=False) # what are the important features?
print(imp_features)

#using Grid Search (method#2) with best feature
categorical_features = ['Race', 'Marital Status', 'T Stage ', 'N Stage', '6th Stage', 'differentiate', 'Grade']
X2 = data2 = get_dummies(data, columns = categorical_features, drop_first=True)
Y = data2['Status']
X_scaled = StandardScaler().fit_transform(X2) # scaling
# X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.2, random_state = 100)# splitting
# X_train,Y_train =SMOTE (random_state = 100).fit_resample(X_train,Y_train)# balancing
DT_classifier4 = tree.DecisionTreeClassifier(criterion = 'entropy') # building classfier
depth = {'max_depth': [2,3,4,5,10,15,20,25,30,35]}
grid_search2 = GridSearchCV(estimator=DT_classifier4, param_grid=depth, scoring='recall', cv=5)
grid_search2.fit(X_scaled, Y) # raining, testing , evaluation, ranking.
best_depth = grid_search2.best_params_
print(best_depth)
best_result = grid_search2.best_score_
print(best_result)

# Decision Tree Classifier (method#1) with the best depth and best features
categorical_features = ['Race', 'Marital Status', 'T Stage ', 'N Stage', '6th Stage', 'differentiate', 'Grade']
X3 = data2 = get_dummies(data, columns = categorical_features, drop_first=True)
X_scaled = StandardScaler().fit_transform(X) # scaling
X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.2, random_state = 100)# splitting
X_train,Y_train =SMOTE (random_state = 100).fit_resample(X_train,Y_train)# balancing
DT_classifier5 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 3)# classifier building
DT_classifier5.fit(X_train, Y_train) # training
Y_pred3 = DT_classifier5.predict(X_test) # testing
imp_features = Series(DT_classifier5.feature_importances_, index=list(X)).sort_values(ascending=False) # what are the important features?
print(imp_features)
# Acuracy and confusion matrix
from sklearn import metrics
Accuracy=metrics.accuracy_score(Y_test, Y_pred3) # calculating accuaracy
print("Accuracy: ", Accuracy) # Is this a good metric??
con_matrix = metrics.confusion_matrix(Y_test, Y_pred3)
print (con_matrix)
recall = metrics.recall_score(Y_test, Y_pred3)
print (recall)
percision=metrics.precision_score(Y_test, Y_pred3)
print(percision)